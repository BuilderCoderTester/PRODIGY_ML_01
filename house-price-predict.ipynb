{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12731453,"sourceType":"datasetVersion","datasetId":8047271}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest, f_regression\nimport warnings\n# Suppress convergence warnings for cleaner output\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/houseprice/train.csv')\ntest_df = pd.read_csv('/kaggle/input/houseprice/train.csv')\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\nprint(f\"\\nTrain columns: {list(train_df.columns)}\")\n\n# Check for missing values\nprint(f\"\\nMissing values in training data:\")\nmissing_train = train_df.isnull().sum()\nprint(missing_train[missing_train > 0].sort_values(ascending=False))\n\n# Target variable analysis\nprint(f\"\\nTarget variable (SalePrice) statistics:\")\nprint(train_df['SalePrice'].describe())\n\n# Log transform target for better distribution (common for price data)\ny = np.log1p(train_df['SalePrice'])\n\n# Feature engineering based on actual Ames dataset columns\ndef feature_engineering(df):\n    df = df.copy()\n    \n    # Total area features (these columns exist in Ames dataset)\n    if all(col in df.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    \n    # Bathroom features\n    bathroom_cols = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n    if all(col in df.columns for col in bathroom_cols):\n        df['TotalBath'] = df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n    \n    # Porch area features  \n    porch_cols = ['OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF']\n    existing_porch_cols = [col for col in porch_cols if col in df.columns]\n    if existing_porch_cols:\n        df['TotalPorch'] = df[existing_porch_cols].sum(axis=1)\n    \n    # Binary indicators\n    if 'PoolArea' in df.columns:\n        df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n    if 'GarageArea' in df.columns:\n        df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n    if 'TotalBsmtSF' in df.columns:\n        df['HasBsmt'] = (df['TotalBsmtSF'] > 0).astype(int)\n    if 'Fireplaces' in df.columns:\n        df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n    \n    # Age features\n    if all(col in df.columns for col in ['YrSold', 'YearBuilt']):\n        df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n    if all(col in df.columns for col in ['YrSold', 'YearRemodAdd']):\n        df['RemodAge'] = df['YrSold'] - df['YearRemodAdd']\n    if 'HouseAge' in df.columns:\n        df['IsNew'] = (df['HouseAge'] <= 2).astype(int)\n    \n    # Quality interactions (these are key features in Ames dataset)\n    if all(col in df.columns for col in ['OverallQual', 'GrLivArea']):\n        df['OverallQual_x_GrLivArea'] = df['OverallQual'] * df['GrLivArea']\n    if all(col in df.columns for col in ['OverallQual', 'TotalSF']):\n        df['OverallQual_x_TotalSF'] = df['OverallQual'] * df['TotalSF']\n    \n    # Handle specific Ames dataset issues\n    # Some categorical variables have 'NA' as a category, not missing values\n    na_as_category = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                      'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                      'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n    \n    for col in na_as_category:\n        if col in df.columns:\n            df[col] = df[col].fillna('None')\n    \n    return df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:41:47.018220Z","iopub.execute_input":"2025-08-16T07:41:47.018520Z","iopub.status.idle":"2025-08-16T07:41:47.071686Z","shell.execute_reply.started":"2025-08-16T07:41:47.018500Z","shell.execute_reply":"2025-08-16T07:41:47.071138Z"}},"outputs":[{"name":"stdout","text":"Train shape: (1460, 81)\nTest shape: (1460, 81)\n\nTrain columns: ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice']\n\nMissing values in training data:\nPoolQC          1453\nMiscFeature     1406\nAlley           1369\nFence           1179\nMasVnrType       872\nFireplaceQu      690\nLotFrontage      259\nGarageType        81\nGarageYrBlt       81\nGarageFinish      81\nGarageQual        81\nGarageCond        81\nBsmtExposure      38\nBsmtFinType2      38\nBsmtQual          37\nBsmtCond          37\nBsmtFinType1      37\nMasVnrArea         8\nElectrical         1\ndtype: int64\n\nTarget variable (SalePrice) statistics:\ncount      1460.000000\nmean     180921.195890\nstd       79442.502883\nmin       34900.000000\n25%      129975.000000\n50%      163000.000000\n75%      214000.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Apply feature engineering\ntrain_processed = feature_engineering(train_df)\ntest_processed = feature_engineering(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:42:13.017865Z","iopub.execute_input":"2025-08-16T07:42:13.018543Z","iopub.status.idle":"2025-08-16T07:42:13.049689Z","shell.execute_reply.started":"2025-08-16T07:42:13.018519Z","shell.execute_reply":"2025-08-16T07:42:13.049169Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Prepare features (remove Id and target)\nX = train_processed.drop(columns=['SalePrice', 'Id'])\nX_test = test_processed.drop(columns=['Id'])\n\n# Ensure same columns in train and test\ncommon_cols = X.columns.intersection(X_test.columns)\nX = X[common_cols]\nX_test = X_test[common_cols]\n\nprint(f\"\\nFeatures after engineering: {X.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:42:38.068721Z","iopub.execute_input":"2025-08-16T07:42:38.069025Z","iopub.status.idle":"2025-08-16T07:42:38.082205Z","shell.execute_reply.started":"2025-08-16T07:42:38.069002Z","shell.execute_reply":"2025-08-16T07:42:38.081448Z"}},"outputs":[{"name":"stdout","text":"\nFeatures after engineering: 91\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify column types\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical columns: {len(num_cols)}\")\nprint(f\"Categorical columns: {len(cat_cols)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:44:33.069543Z","iopub.execute_input":"2025-08-16T07:44:33.069868Z","iopub.status.idle":"2025-08-16T07:44:33.082442Z","shell.execute_reply.started":"2025-08-16T07:44:33.069846Z","shell.execute_reply":"2025-08-16T07:44:33.081822Z"}},"outputs":[{"name":"stdout","text":"Numerical columns: 48\nCategorical columns: 43\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n# Show some actual column names\nprint(f\"\\nSample numerical columns: {num_cols[:10]}\")\nprint(f\"Sample categorical columns: {cat_cols[:10]}\")\n\n# Preprocessing pipelines\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', RobustScaler())  # Better for outliers than StandardScaler\n])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_cols),\n        ('cat', cat_transformer, cat_cols)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:45:31.895865Z","iopub.execute_input":"2025-08-16T07:45:31.896142Z","iopub.status.idle":"2025-08-16T07:45:31.901704Z","shell.execute_reply.started":"2025-08-16T07:45:31.896123Z","shell.execute_reply":"2025-08-16T07:45:31.900984Z"}},"outputs":[{"name":"stdout","text":"\nSample numerical columns: ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2']\nSample categorical columns: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n\n# Base models - ONLY LINEAR REGRESSION variants for ensemble\n# Different regularization approaches but all linear models\nbase_models = [\n    ('linear', LinearRegression()),\n    ('ridge', RidgeCV(alphas=np.logspace(-3, 2, 30), cv=5)),\n    ('lasso', LassoCV(alphas=np.logspace(-3, 1, 30), cv=5, max_iter=5000, tol=1e-3)),\n    ('elastic', ElasticNetCV(alphas=np.logspace(-3, 1, 20), \n                            l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95], \n                            cv=5, max_iter=5000, tol=1e-3))\n]\n\n# Advanced ensemble: Stacking with linear regression meta-learner\nstacking_regressor = Pipeline([\n    ('preprocessor', preprocessor),\n    ('stacking', StackingRegressor(\n        estimators=base_models,\n        final_estimator=LinearRegression(),\n        cv=5,\n        n_jobs=-1\n    ))\n])\n\n# Alternative: Simple averaging ensemble\nvoting_regressor = Pipeline([\n    ('preprocessor', preprocessor),\n    ('voting', VotingRegressor(\n        estimators=base_models,\n        n_jobs=-1\n    ))\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:47:37.015862Z","iopub.execute_input":"2025-08-16T07:47:37.016567Z","iopub.status.idle":"2025-08-16T07:47:37.023247Z","shell.execute_reply.started":"2025-08-16T07:47:37.016543Z","shell.execute_reply":"2025-08-16T07:47:37.022618Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# Cross-validation evaluation\ndef evaluate_model(model, X, y, cv=5):\n    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n    scores = cross_val_score(model, X, y, cv=kfold, \n                           scoring='neg_mean_squared_error', n_jobs=-1)\n    return np.sqrt(-scores)  # Convert to RMSE\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CROSS-VALIDATION EVALUATION\")\nprint(\"=\"*60)\n\n# Evaluate models\nprint(\"\\nEvaluating Stacking Regressor (Linear + Ridge + Lasso + ElasticNet)...\")\nstacking_scores = evaluate_model(stacking_regressor, X_train, y_train)\nprint(f\"Stacking CV RMSE: {stacking_scores.mean():.4f} (+/- {stacking_scores.std() * 2:.4f})\")\n\nprint(\"\\nEvaluating Voting Regressor...\")\nvoting_scores = evaluate_model(voting_regressor, X_train, y_train)\nprint(f\"Voting CV RMSE: {voting_scores.mean():.4f} (+/- {voting_scores.std() * 2:.4f})\")\n\n# Train the final model\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING FINAL MODEL\")\nprint(\"=\"*60)\n\n# Use the better performing model\nif stacking_scores.mean() < voting_scores.mean():\n    final_model = stacking_regressor\n    model_name = \"Stacking Regressor\"\nelse:\n    final_model = voting_regressor\n    model_name = \"Voting Regressor\"\n\nprint(f\"\\nUsing {model_name} as final model\")\nfinal_model.fit(X_train, y_train)\n\n# Validation predictions\ny_val_pred = final_model.predict(X_val)\nval_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nval_mae = mean_absolute_error(y_val, y_val_pred)\n\nprint(f\"\\nValidation Metrics:\")\nprint(f\"RMSE: {val_rmse:.4f}\")\nprint(f\"MAE: {val_mae:.4f}\")\n\n# Generate test predictions\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING PREDICTIONS\")\nprint(\"=\"*60)\n\ntest_predictions = final_model.predict(X_test)\n# Convert back from log scale\ntest_predictions = np.expm1(test_predictions)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'Id': test_df['Id'],\n    'SalePrice': test_predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\n\nprint(f\"Submission created with {len(submission)} predictions\")\nprint(f\"Price range: ${test_predictions.min():,.0f} - ${test_predictions.max():,.0f}\")\nprint(f\"Mean price: ${test_predictions.mean():,.0f}\")\n\n# Show first few predictions\nprint(f\"\\nFirst 10 predictions:\")\nprint(submission.head(10))\n\n# Model summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL SUMMARY\")\nprint(\"=\"*60)\nprint(f\"✓ Dataset: Ames Housing with {X.shape[1]} features\")\nprint(f\"✓ Target: Log-transformed SalePrice\")\nprint(f\"✓ Base Models: Linear, Ridge, Lasso, ElasticNet regression\")\nprint(f\"✓ Ensemble: {model_name}\")\nprint(f\"✓ Preprocessing: Robust scaling + One-hot encoding\") \nprint(f\"✓ Cross-validation RMSE: {min(stacking_scores.mean(), voting_scores.mean()):.4f}\")\nprint(f\"✓ Validation RMSE: {val_rmse:.4f}\")\nprint(\"✓ Ready for Kaggle submission!\")\n\n# Additional insights\nprint(f\"\\nModel Insights:\")\nprint(f\"• Used {len(num_cols)} numerical and {len(cat_cols)} categorical features\")\nprint(f\"• Log transformation helps with price distribution\")\nprint(f\"• Ensemble combines linear models with different regularization\")\nprint(f\"• Cross-validation ensures robust performance estimates\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:49:01.197202Z","iopub.execute_input":"2025-08-16T07:49:01.197857Z","iopub.status.idle":"2025-08-16T07:49:40.173883Z","shell.execute_reply.started":"2025-08-16T07:49:01.197833Z","shell.execute_reply":"2025-08-16T07:49:40.173055Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nCROSS-VALIDATION EVALUATION\n============================================================\n\nEvaluating Stacking Regressor (Linear + Ridge + Lasso + ElasticNet)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [5, 9, 14, 16, 26, 38] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [22, 27] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [10, 15, 29] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [8, 10, 15] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [10, 15, 16, 19, 31, 35, 36, 40] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Stacking CV RMSE: 19215.4360 (+/- 47409.8394)\n\nEvaluating Voting Regressor...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [5, 9, 14, 16, 26, 38] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [22, 27] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [10, 15, 29] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [8, 10, 15] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [10, 15, 16, 19, 31, 35, 36, 40] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Voting CV RMSE: 11752.2496 (+/- 29351.1865)\n\n============================================================\nTRAINING FINAL MODEL\n============================================================\n\nUsing Voting Regressor as final model\n\nValidation Metrics:\nRMSE: 244.5080\nMAE: 19.4654\n\n============================================================\nGENERATING PREDICTIONS\n============================================================\nSubmission created with 1460 predictions\nPrice range: $47,611 - $inf\nMean price: $inf\n\nFirst 10 predictions:\n   Id      SalePrice\n0   1  208744.397106\n1   2  198919.880255\n2   3  218620.236644\n3   4  168733.073957\n4   5  296032.993874\n5   6  156248.435923\n6   7  275162.185225\n7   8  212350.659869\n8   9  133867.887691\n9  10  117294.026676\n\n============================================================\nMODEL SUMMARY\n============================================================\n✓ Dataset: Ames Housing with 91 features\n✓ Target: Log-transformed SalePrice\n✓ Base Models: Linear, Ridge, Lasso, ElasticNet regression\n✓ Ensemble: Voting Regressor\n✓ Preprocessing: Robust scaling + One-hot encoding\n✓ Cross-validation RMSE: 11752.2496\n✓ Validation RMSE: 244.5080\n✓ Ready for Kaggle submission!\n\nModel Insights:\n• Used 48 numerical and 43 categorical features\n• Log transformation helps with price distribution\n• Ensemble combines linear models with different regularization\n• Cross-validation ensures robust performance estimates\n","output_type":"stream"}],"execution_count":15}]}